{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import SementicRoleLabeller\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentor = Segmentor()  # 初始化实例\n",
    "# segmentor.load('../ltp_data/cws.model')  # 加载模型\n",
    "# postagger = Postagger() # 初始化实例\n",
    "# postagger.load('../ltp_data/pos.model')  # 加载模型\n",
    "# parser = Parser() # 初始化实例\n",
    "# parser.load('../ltp_data/parser.model')  # 加载模型\n",
    "\n",
    "# # print (\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "# # parser.release()  # 释放模型\n",
    "# def segment_pos(sentence):\n",
    "#     words = segmentor.segment(sentence)  # 分词\n",
    "#     words_list = list(words)\n",
    "#     postags = postagger.postag(words)  # 词性标注\n",
    "#     word_pos=[]\n",
    "#     arcs = parser.parse(words, postags)  # 句法分析\n",
    "#     for it in zip(words_list,postags,arcs):\n",
    "#         word_pos.append((it[0],it[1],it[2].head,it[2].relation))\n",
    "# #     print (\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "    \n",
    "#     # postagger.release()  # 释放模型\n",
    "#     # segmentor.release()  # 释放模型\n",
    "#     return word_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_pos('收到了，太实惠了，买了一大箱，以后继续购买，送货速度快服务也好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "import json\n",
    "with open('tmp_train.json') as f:\n",
    "    col_list=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame()\n",
    "for index,it in enumerate(col_list):\n",
    "    train[index]=it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns=['content','theme','sentiment','tokens','postags']\n",
    "# train.columns=['content','tokens','postags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_seg=[]\n",
    "for i in range(train.shape[0]):\n",
    "    token=train.loc[i]['tokens']\n",
    "    postag=train.loc[i]['postags']\n",
    "    sent_pos=[]\n",
    "    for ind in range(len(token)):\n",
    "        sent_pos.append([(to,po) for to,po in zip(token[ind],postag[ind])])\n",
    "    orig_seg.append(sent_pos)\n",
    "train['orig_seg']=orig_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl=[]\n",
    "for row in train['theme']:\n",
    "    word=row.split(';')[:-1]\n",
    "    wlen=[len(w) for w in word if w !='NULL']\n",
    "    wl.extend(wlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1091, 2: 17931, 3: 1763, 4: 1216, 5: 12, 6: 4})"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "with open('tmp_test.json') as f:\n",
    "    col_list=json.load(f)\n",
    "test=pd.DataFrame(col_list[0])\n",
    "for index,it in enumerate(col_list[1:]):\n",
    "    test[index]=it\n",
    "test.columns=['content','tokens','postags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 3)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_seg=[]\n",
    "for i in range(test.shape[0]):\n",
    "    try:\n",
    "        token=test.loc[i]['tokens']\n",
    "        postag=test.loc[i]['postags']\n",
    "        sent_pos=[]\n",
    "        for ind in range(len(token)):\n",
    "            sent_pos.append([(to,po) for to,po in zip(token[ind],postag[ind])])\n",
    "        orig_seg.append(sent_pos)\n",
    "    except:\n",
    "        print(token)\n",
    "test['orig_seg']=orig_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('比', 'p'), ('黑人', 'n'), ('碳头', 'n'), ('牙刷', 'n'), ('好', 'a'), ('用', 'v')]]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_seg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(row):\n",
    "    word2label=[]\n",
    "    orig_seg=row['orig_seg']\n",
    "    try:\n",
    "        theme=row['theme'].split(';')[:-1]\n",
    "        sentiment_word=row['sentiment'].split(';')[:-1]\n",
    "        theme.extend(sentiment_word)\n",
    "        will_del='\\t'.join([it for it in theme if it !='NULL'])\n",
    "        for sub_sent in orig_seg:\n",
    "            tmp_lb=[]\n",
    "            for word in sub_sent:\n",
    "                if word[0] in will_del:\n",
    "                    tmp_lb.append((word[0],word[1],'N'))\n",
    "                else:\n",
    "                    tmp_lb.append((word[0],word[1],'Y'))\n",
    "            word2label.append(tmp_lb)\n",
    "    except:\n",
    "        for sub_sent in orig_seg:\n",
    "            tmp_lb=[]\n",
    "            for word in sub_sent:\n",
    "                    tmp_lb.append((word[0],word[1],'Y'))\n",
    "            word2label.append(tmp_lb)\n",
    "    return word2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for i in range(train.shape[0]):\n",
    "    try:\n",
    "        labels.append(get_labels(train.loc[i]))\n",
    "    except:\n",
    "        labels.append([])\n",
    "train['labels']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('收到', 'v', 'Y'), ('了', 'u', 'Y')],\n",
       " [('太', 'd', 'Y'), ('实惠', 'a', 'N'), ('了', 'u', 'Y')],\n",
       " [('买', 'v', 'Y'),\n",
       "  ('了', 'u', 'Y'),\n",
       "  ('一', 'm', 'Y'),\n",
       "  ('大', 'a', 'Y'),\n",
       "  ('箱', 'q', 'Y')],\n",
       " [('以后', 'nd', 'Y'), ('继续', 'v', 'Y'), ('购买', 'v', 'Y')],\n",
       " [('送货', 'v', 'N'),\n",
       "  ('速度', 'n', 'N'),\n",
       "  ('快', 'a', 'N'),\n",
       "  ('服务', 'v', 'N'),\n",
       "  ('也好', 'u', 'N')]]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('resource/perception.txt',encoding='utf-8') as f:\n",
    "    percept_words=[line.strip() for line in f]\n",
    "percept_words=set(percept_words)\n",
    "def perception(word):\n",
    "    if word in percept_words:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('resource/polarity_words_1.txt',encoding='utf-8') as f:\n",
    "    polar_words1=[line.strip() for line in f]\n",
    "with codecs.open('resource/polarity_words_2.txt',encoding='utf-8') as f:\n",
    "    polar_words2=[line.strip() for line in f]\n",
    "polar_words1.extend(polar_words2)\n",
    "polar_word=set(polar_words1)\n",
    "def polarword(word):\n",
    "    if word in polar_word:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('resource/word2cluster.json') as f:\n",
    "    word2cluster=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cluster(word):\n",
    "    try:\n",
    "        cluster=word2cluster[word]\n",
    "    except KeyError:\n",
    "        cluster='0000'\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('resource/baike-50.vec.txt',encoding='utf-8') as f:\n",
    "    word2emb={}\n",
    "    for line in f:\n",
    "        items=line.strip().split()\n",
    "        emb=[float(it) for it in items[1:]]\n",
    "        try:\n",
    "            word2emb[items[0]]=emb\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embed(word):\n",
    "    embed={}\n",
    "    try:\n",
    "        for ind,it in enumerate(word2emb[word]):\n",
    "            embed[str(ind)]=it\n",
    "    except:\n",
    "        pass\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2feature(sent,i):\n",
    "    sent_len=len(sent)\n",
    "    word=sent[i][0]\n",
    "    postag=sent[i][1]\n",
    "    features={\n",
    "        'word':word,\n",
    "        'pref:word':word[0],\n",
    "        'suff:word':word[-1],\n",
    "        'postag': postag,\n",
    "        'perception':perception(word),\n",
    "        'polar_word':polarword(word),\n",
    "        'cluster':word_cluster(word),\n",
    "\n",
    "        # negative effect    \n",
    "#         'word_head':sent[i][2],\n",
    "#         'word_relation':sent[i][3],\n",
    "    }\n",
    "    if i>0 and i<sent_len-1:\n",
    "        # about word\n",
    "        word1=sent[i-1][0]\n",
    "        word2=sent[i+1][0]\n",
    "        features.update({\n",
    "            '-1:word':word1,\n",
    "            '+1:word':word2,\n",
    "            '-1+word':word1+'|'+word,\n",
    "            '+1+word':word+'|'+word2,\n",
    "        })\n",
    "        # about tag\n",
    "        postag1=sent[i-1][1]\n",
    "        postag2=sent[i+1][1]\n",
    "        features.update({\n",
    "            '-1:postag': postag1,\n",
    "            '+1:postag': postag2,\n",
    "            '-1+postag':postag1+'|'+postag,\n",
    "            '+1+posttag':postag+'|'+postag2,\n",
    "        })\n",
    "    elif i==0 and len(sent)>1:\n",
    "        # about word\n",
    "        word2=sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word':word2,\n",
    "            '+1+word':word+'|'+word2,\n",
    "        })\n",
    "        # about tag\n",
    "        postag2=sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:postag': postag2,\n",
    "            '+1+posttag':postag+'|'+postag2,\n",
    "        })\n",
    "        features['EOS']=True\n",
    "    elif len(sent)>1:\n",
    "        # about word\n",
    "        word1=sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word':word1,\n",
    "            '-1+word':word1+'|'+word,\n",
    "        })\n",
    "        # about tag\n",
    "        postag1=sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:postag': postag1,\n",
    "            '-1+postag':postag1+'|'+postag,\n",
    "        })\n",
    "        features['END']=True\n",
    "    else:\n",
    "        features['EOS']=True\n",
    "        features['END']=True\n",
    "    features.update(add_embed(word))\n",
    "    return features\n",
    "def sent2features(sent):\n",
    "    return [word2feature(sent,i) for  i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [it[2] for it in sent]\n",
    "# def sent2token(sent):\n",
    "#     return [it[0] for it in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent=0.8\n",
    "data_num=taiyi_train.shape[0]\n",
    "train_num=int(data_num*percent)\n",
    "random.shuffle(labels)\n",
    "train_sents=labels[:train_num]\n",
    "test_sents=labels[train_num:]\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "for sub_sent in train_sents:\n",
    "    for sent in sub_sent:\n",
    "        x_train.append(sent2features(sent))\n",
    "        y_train.append(sent2labels(sent))\n",
    "# x_train=[sent2features(s) for s in train_sents]\n",
    "# y_train=[sent2labels(s) for s in train_sents]\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "for sub_sent in test_sents:\n",
    "    for sent in sub_sent:\n",
    "        x_test.append(sent2features(sent))\n",
    "        y_test.append(sent2labels(sent))    \n",
    "# x_test=[sent2features(s) for s in test_sents]\n",
    "# y_test=[sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf=sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "#     c1=0.20,\n",
    "#     c2=0.14,\n",
    "    c1=1,\n",
    "    c2=2,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    "                        )\n",
    "crf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          N      0.885     0.764     0.820     18678\n",
      "          Y      0.926     0.968     0.946     57210\n",
      "\n",
      "avg / total      0.916     0.917     0.915     75888\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_list=list(crf.classes_)\n",
    "y_pred=crf.predict(x_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, label_list, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# with open('model/CRF_all.mdl','wb') as f:\n",
    "#     joblib.dump(crf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('按键', 'n', 3, 'ATT', 'N'), ('反应', 'v', 3, 'ATT', 'N'), ('速度', 'n', 4, 'SBV', 'N'), ('有', 'v', 0, 'HED', 'N'), ('一点', 'm', 6, 'ADV', 'N'), ('慢', 'a', 4, 'VOB', 'N')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['N', 'N', 'N', 'N', 'N', 'N']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('model/CRF.mdl','rb') as f:\n",
    "#     crf=joblib.load(f)\n",
    "\n",
    "# print(labels[0])\n",
    "# crf.predict_single(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fine tuning param\n",
    "# # %%time\n",
    "# crf=sklearn_crfsuite.CRF(algorithm='lbfgs',\n",
    "#                         max_iterations=100,\n",
    "#                         all_possible_transitions=True)\n",
    "# params_space={\n",
    "#     'c1':np.linspace(0,10,10),\n",
    "#     'c2':np.linspace(0,10,10),\n",
    "# }\n",
    "# f1_score=make_scorer(metrics.flat_f1_score,average='weighted',labels=label_list)\n",
    "# rs=GridSearchCV(crf,\n",
    "#                      params_space,\n",
    "#                      cv=5,\n",
    "#                      verbose=1,\n",
    "#                      n_jobs=-1,\n",
    "# #                      n_iter=100,\n",
    "# #                      scoring=f1_score\n",
    "#                )\n",
    "# rs.fit(x_train,y_train)\n",
    "# print('best params:', rs.best_params_)\n",
    "# print('best CV score:', rs.best_score_)\n",
    "# print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "# crf = rs.best_estimator_\n",
    "# y_pred = crf.predict(x_test)\n",
    "# print(metrics.flat_classification_report(\n",
    "#     y_test, y_pred, label_list, digits=3\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_list=re.split('，|。|？|！',sent)\n",
    "def sent_split(sent):\n",
    "    sent_list=re.split('，|。|？|！',sent)\n",
    "    sent_list=[it  for it in sent_list if len(it)>1]\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['清洁功能挺好', '以前就算刚洗完头发不久就有粉粉的头屑', '用这个洗就一点都没有了']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list=sent_split(taiyi_train.loc[24][1])\n",
    "sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_compress(row):\n",
    "#     sent_label=[]\n",
    "    sent_comp=[]\n",
    "    sent_feature=sent2features(row)\n",
    "    label=crf.predict_single(sent_feature)\n",
    "#     sent_label.append(label)\n",
    "    for ind,lb in enumerate(label):\n",
    "        if lb == 'N':\n",
    "            sent_comp.append(row[ind][0])\n",
    "    return (sent_comp,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set \n",
    "sent_press_list=[]\n",
    "sent_label_list=[]\n",
    "for ind,row in enumerate(train['orig_seg']):\n",
    "    try:\n",
    "        sent_press=[]\n",
    "        sent_label=[]\n",
    "        for sub_sent in row:\n",
    "            result=sent_compress(sub_sent)\n",
    "            sent_press.append(result[0])\n",
    "            sent_label.append(result[1])\n",
    "        sent_press_list.append(sent_press)\n",
    "        sent_label_list.append(sent_label)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "train['sent_compress']=sent_press_list\n",
    "train['sent_label']=sent_label_list\n",
    "\n",
    "# taiyi_train.drop('row_id',axis=1,inplace=True)\n",
    "# taiyi_train.to_csv('taiyi_train_compress.csv',index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set \n",
    "sent_press_list=[]\n",
    "sent_label_list=[]\n",
    "for ind,row in enumerate(test['orig_seg']):\n",
    "    try:\n",
    "        sent_press=[]\n",
    "        sent_label=[]\n",
    "        for sub_sent in row:\n",
    "            result=sent_compress(sub_sent)\n",
    "            sent_press.append(result[0])\n",
    "            sent_label.append(result[1])\n",
    "        sent_press_list.append(sent_press)\n",
    "        sent_label_list.append(sent_label)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "test['sent_compress']=sent_press_list\n",
    "test['sent_label']=sent_label_list\n",
    "\n",
    "# taiyi_train.drop('row_id',axis=1,inplace=True)\n",
    "# taiyi_train.to_csv('taiyi_train_compress.csv',index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_press_list[0],sent_label_l\n",
    "with open('sent_label_test.json','w') as f:\n",
    "    json.dump(sent_label_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>postags</th>\n",
       "      <th>orig_seg</th>\n",
       "      <th>sent_compress</th>\n",
       "      <th>sent_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>比黑人碳头牙刷好用?</td>\n",
       "      <td>[[比, 黑人, 碳头, 牙刷, 好, 用]]</td>\n",
       "      <td>[[p, n, n, n, a, v]]</td>\n",
       "      <td>[[(比, p), (黑人, n), (碳头, n), (牙刷, n), (好, a), (...</td>\n",
       "      <td>[[牙刷, 好]]</td>\n",
       "      <td>[[Y, Y, Y, N, N, Y]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>比之前坏的要好些，亮一些。</td>\n",
       "      <td>[[比, 之前, 坏, 的, 要, 好, 些], [亮, 一些]]</td>\n",
       "      <td>[[p, nd, a, u, v, a, q], [v, m]]</td>\n",
       "      <td>[[(比, p), (之前, nd), (坏, a), (的, u), (要, v), (好...</td>\n",
       "      <td>[[坏, 好], []]</td>\n",
       "      <td>[[Y, Y, N, Y, Y, N, Y], [Y, Y]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content                             tokens  \\\n",
       "0     比黑人碳头牙刷好用?            [[比, 黑人, 碳头, 牙刷, 好, 用]]   \n",
       "1  比之前坏的要好些，亮一些。  [[比, 之前, 坏, 的, 要, 好, 些], [亮, 一些]]   \n",
       "\n",
       "                            postags  \\\n",
       "0              [[p, n, n, n, a, v]]   \n",
       "1  [[p, nd, a, u, v, a, q], [v, m]]   \n",
       "\n",
       "                                            orig_seg sent_compress  \\\n",
       "0  [[(比, p), (黑人, n), (碳头, n), (牙刷, n), (好, a), (...     [[牙刷, 好]]   \n",
       "1  [[(比, p), (之前, nd), (坏, a), (的, u), (要, v), (好...  [[坏, 好], []]   \n",
       "\n",
       "                        sent_label  \n",
       "0             [[Y, Y, Y, N, N, Y]]  \n",
       "1  [[Y, Y, N, Y, Y, N, Y], [Y, Y]]  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_excel('taiyi_train_compress.xls',index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set\n",
    "sent_press=[]\n",
    "sent_no_press=[]\n",
    "sent_label=[]\n",
    "for ind,row in enumerate(taiyi_test[taiyi_test.columns[1]]):\n",
    "    try:\n",
    "        result=sent_compress(row)\n",
    "        sent_press.append(result[0])\n",
    "        sent_no_press.append(result[1])\n",
    "        sent_label.append(result[2])\n",
    "    except:\n",
    "        sent_press.append([])\n",
    "        sent_no_press.append([])\n",
    "        sent_label.append([])\n",
    "taiyi_test['sent_compress']=sent_press\n",
    "taiyi_test['sent_no_press']=sent_no_press\n",
    "taiyi_test['sent_label']=sent_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taiyi_test.drop('row_id',axis=1,inplace=True)\n",
    "taiyi_test.to_excel('taiyi_test_compress.xls',index=None,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}